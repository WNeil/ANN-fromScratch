{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f027e162-1016-4255-988d-fb567cccc38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing the data, this will take a while...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Back_propogate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 168\u001b[0m\n\u001b[0;32m    165\u001b[0m y_train_hot \u001b[38;5;241m=\u001b[39m one_hot(y_train)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m#finally, call the program\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m trained_weights_input_hidden, trained_biases_input_hidden, trained_weights_hidden_output, trained_biases_hidden_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_neural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m#stats for configs:\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m#baseline: 30 layers, 85 epochs, 500 batches, .01 learn rate\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m#Overall Accuracy: 0.9396823280388007 -baseline,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m#    each epoch contributes much less accuracy in this learning rate, implying its too low a number\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 119\u001b[0m, in \u001b[0;36mtrain_neural_network\u001b[1;34m(X_train, Y_train, batch_size, input_size, hidden_size, output_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m    116\u001b[0m hidden_output, output_output \u001b[38;5;241m=\u001b[39m forward_propogation(X_shuffle[i], weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m#backwards propogation\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[43mBack_propogate\u001b[49m(X_shuffle[i], Y_shuffle[i], hidden_output, output_output, weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, learning_rate)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m#passing X_shuffle into the forward prop is using the entire dataset, which obviously cant be used\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m#calculating info loss for the batch\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Back_propogate' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "testing_data = pd.read_csv(\"assets/testing10000.csv\")\n",
    "testing_data_labels = pd.read_csv(\"assets/testing10000_labels.csv\")\n",
    "\n",
    "training_data = pd.read_csv(\"assets/training60000.csv\")\n",
    "training_data_labels = pd.read_csv(\"assets/training60000_labels.csv\")\n",
    "\n",
    "#print(\"training data labels dimensions look like: \")\n",
    "#print(training_data_labels.shape)\n",
    "#print(\"The first 5 rows of the original labels: \")\n",
    "#print(training_data_labels[:5])\n",
    "\n",
    "\n",
    "#need functions to pull data from the csv, pass 2 dataframes\n",
    "def load_from_csv(features, labels):\n",
    "    X = features.values\n",
    "    y = labels.values\n",
    "    return X, y\n",
    "\n",
    "#for final accuracy calc\n",
    "def calculate_accuracy(predictions, ground_truth):\n",
    "    correct_predictions = np.sum(predictions == ground_truth)\n",
    "    total_samples = len(ground_truth)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy * 100\n",
    "\n",
    "#logistic activation function, also known as sigmoid\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#derivative of the logistic function as well\n",
    "def logistic_deriv(x):\n",
    "    return x * (1-x)\n",
    "\n",
    "#softmax activation function\n",
    "def softmax(x):\n",
    "    exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "#mean squared error for the cost of prediction\n",
    "def mean_squared_error(true, prediction):\n",
    "    return np.mean((true, prediction)**2)\n",
    "\n",
    "#and the derivative of the function\n",
    "def mean_squared_error_deriv(true, prediction):\n",
    "    return 2 * (prediction - true) / len(true)\n",
    "\n",
    "#cross entropy\n",
    "def cross_entropy_loss(true, prediction):\n",
    "    #epsilon storage\n",
    "    epsilon = 1e-8\n",
    "    prediction = np.clip(prediction, epsilon, 1 - epsilon)\n",
    "    return -np.sum(true * np.log(prediction)) / len(true)\n",
    "\n",
    "#and the cross entropy derivative\n",
    "def cross_entropy_loss_deriv(true, prediction):\n",
    "    return (prediction - true) / len(true)\n",
    "\n",
    "\n",
    "\n",
    "def initialize_weights(input, hidden, output):\n",
    "    #maybe have to set a random seed here? keep it consistent\n",
    "    np.random.seed(42)\n",
    "    weights_input_hidden = np.random.randn(input, hidden)\n",
    "    biases_input_hidden = np.zeros((1, hidden))\n",
    "    weights_hidden_output = np.random.randn(hidden, output)\n",
    "    biases_hidden_output = np.zeros((1, output))\n",
    "    return weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output\n",
    "\n",
    "\n",
    "def forward_propogation(X, weights_input, bias_input, weights_hidden_output, biases_hidden_output):\n",
    "    #forward pass\n",
    "    hidden_input = np.dot(X, weights_input) + bias_input\n",
    "    hidden_output = logistic(hidden_input) #use the activation function on the dot product\n",
    "    output_input = np.dot(hidden_output, weights_hidden_output) + biases_hidden_output #for next layer, dot product the activated layer and the weights + bias\n",
    "    output_output = softmax(output_input) #apply softmax to change the values to probabilities, 0.0-1.0\n",
    "    \n",
    "    return hidden_output, output_output #return output of the hidden layer, as well as the probability output we continue passing \n",
    "    \n",
    "\n",
    "#backpropogation\n",
    "def Back_propogate(X, y, hidden_output, output_output, weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, learning_rate):\n",
    "\n",
    "    #find the derivative of the error of the output layer, first by output with cross entropy and then the dot product of the output error with the weights\n",
    "    #to find what we need to change, multiplying it by the deriv of the sigmoid func\n",
    "    output_error = cross_entropy_loss_deriv(y, output_output)\n",
    "    hidden_error = np.dot(output_error, weights_hidden_output.T) * logistic_deriv(hidden_output)\n",
    "\n",
    "    #weight updates with the learning rate\n",
    "    weights_hidden_output -= learning_rate * np.dot(hidden_output.T, output_error)\n",
    "    biases_hidden_output -= learning_rate * np.sum(output_error, axis=0, keepdims=True)\n",
    "    weights_input_hidden -= learning_rate * np.dot(X.reshape(1, -1).T, hidden_error)\n",
    "    biases_input_hidden -= learning_rate * np.sum(hidden_error, axis=0, keepdims=True)\n",
    "\n",
    "def train_neural_network(X_train, Y_train, batch_size, input_size, hidden_size, output_size, learning_rate, epochs):\n",
    "    #generate the weights and biases\n",
    "    weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    #printed line just to know we entered and that data is being processed\n",
    "    print(\"now processing the data, this will take a while...\")\n",
    "    \n",
    "    #for tracking and printing\n",
    "    correct_classifs = 0\n",
    "    incorrect_classifs = 0\n",
    "\n",
    "    #start doing training loops\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(len(X_train)) #create a var that spans the length of the X training set\n",
    "        np.random.shuffle(indices) #shuffle those indices\n",
    "        X_shuffle, Y_shuffle = X_train[indices], Y_train[indices] #take a data nugget from the y and x training set\n",
    "\n",
    "        for i in range(len(X_shuffle)):\n",
    "            #forward propogation\n",
    "            hidden_output, output_output = forward_propogation(X_shuffle[i], weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "            \n",
    "            #backwards propogation\n",
    "            Back_propogate(X_shuffle[i], Y_shuffle[i], hidden_output, output_output, weights_input_hidden, weights_hidden_output, biases_input_hidden, biases_hidden_output, learning_rate)\n",
    "            \n",
    "            #passing X_shuffle into the forward prop is using the entire dataset, which obviously cant be used\n",
    "            #calculating info loss for the batch\n",
    "            if i % batch_size == 0:\n",
    "                _, train_output = forward_propogation(X_shuffle[i], weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "                loss = cross_entropy_loss(Y_train, train_output)\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Batch {i//batch_size + 1}/{len(X_shuffle)//batch_size}, Loss: {loss}\")\n",
    "\n",
    "    _, final_output = forward_propogation(X_train, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "    predicted_labels = np.argmax(final_output, axis=1)\n",
    "    ground_truth_labels = np.argmax(Y_train, axis=1)\n",
    "\n",
    "\n",
    "    #print out the params of the process\n",
    "    print(f\"Output of a neural network with {hidden_size} neurons in a single hidden layer, a learning rate of {learning_rate}, data divided into {batch_size} and passed over {epochs} times:\")\n",
    "    \n",
    "    #count the classifs\n",
    "    correct_classifs += np.sum(predicted_labels == ground_truth_labels)\n",
    "    incorrect_classifs += np.sum(predicted_labels != ground_truth_labels)\n",
    "    print(f\"Correct classifications made: {correct_classifs}\")\n",
    "    print(f\"Incorrect classifications made: {incorrect_classifs}\")\n",
    "\n",
    "    # No need for one-hot encoding for ground truth labels in the accuracy calculation\n",
    "    accuracy = calculate_accuracy(predicted_labels, ground_truth_labels)\n",
    "    print(f\"Overall Accuracy: {accuracy}%\")\n",
    "\n",
    "    return weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output\n",
    "\n",
    "    \n",
    "x_train, y_train = load_from_csv(training_data, training_data_labels)\n",
    "\n",
    "#details of the ANN outlined by assignment + experiment numbers\n",
    "inputs = 784 #logistic\n",
    "hidden_layers = 30 #can change the number to experiment\n",
    "output_layers = 10 #softmax\n",
    "epochs = 25\n",
    "batches = 500 #batch length, make it much larger for the actual data\n",
    "learn_rate = .01\n",
    "\n",
    "#gotta \"one hot\" the y graph to remove errors\n",
    "def one_hot(y):\n",
    "    one_hot_y = np.zeros((y.size, y.max() + 1))\n",
    "    one_hot_y[np.arange(y.size), y.flatten()] = 1  # Use flatten to ensure correct indexing\n",
    "    return one_hot_y\n",
    "    \n",
    "y_train_hot = one_hot(y_train)\n",
    "\n",
    "#finally, call the program\n",
    "trained_weights_input_hidden, trained_biases_input_hidden, trained_weights_hidden_output, trained_biases_hidden_output = train_neural_network(x_train, y_train_hot, batches, inputs, hidden_layers, output_layers, learn_rate, epochs) \n",
    "\n",
    "#stats for configs:\n",
    "#baseline: 30 layers, 85 epochs, 500 batches, .01 learn rate\n",
    "#Overall Accuracy: 0.9396823280388007 -baseline,\n",
    "#Overall Accuracy: 0.9345655760929349 -15 epochs\n",
    "#Overall Accuracy: 0.9185153085884765 -45 epochs\n",
    "#Overall Accuracy: 0.9090151502525042 -55 epochs\n",
    "#Overall Accuracy: 0.89504825080418 -65 epochs\n",
    "#Overall Accuracy: 0.9027983799729995 -60 epochs, the magic number\n",
    "\n",
    "#proceeding with 25 epochs:\n",
    "# 250 batches = no difference in the loss jumping\n",
    "# .1 learning rate = massive jump in percent, not in the loss jumping\n",
    "#    proceeding to push down the pochs with learn rate at this value:\n",
    "#    20 epochs = Overall Accuracy: 0.9587326455440924 \n",
    "#    10 epochs = Overall Accuracy: 0.9430157169286155\n",
    "#    1 epoch? = Overall Accuracy: 0.8652144202403373 \n",
    "#\n",
    "#    this is likely an overshooting issue, time to see learning rate .001 impact:\n",
    "#    20 epochs = Overall Accuracy: 0.7585126418773647 \n",
    "#    30 epochs = Overall Accuracy: 0.797046617443624 \n",
    "#    85 epoochs = Overall Accuracy: 0.8594476574609576\n",
    "#    the least necessary epochs for the model to have 90% accuracy is much higher than .01,  \n",
    "#    each epoch contributes much less accuracy in this learning rate, implying its too low a number\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f03108-4eb5-421f-9fe1-eda1c29293fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
